{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py, tifffile\n",
    "import numpy as np\n",
    "import random, cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"../scripts/data_preparation\")  \n",
    "label_shape = (100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "This cell should contain everything that you might need to change, with the one exception being adding datasets, in which case, in cell 3 you'd need to update the filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/home/brian/data4/brian/PBnJ/jelly_h5s/RFamide/mixed_datasets\"\n",
    "\n",
    "raw = True # If true, we will make the h5s from raw tif files provided\n",
    "frames_per_dataset = 100 # How many frames from each dataset we should sample\n",
    "\n",
    "add_extra = False # Should we add more frames from an nrrd to augment the labeled frames\n",
    "num_frames_to_add = 500 # If we add extra frames, how many should we add\n",
    "manually_selected_frames = [] # A list of the frames you want to make sure get included from this dataset\n",
    "padding = \"double\" # \"single\" or \"double\" - Do you want to pad on both the top and bottom or just the bottom\n",
    "side_len = 1024 # What the cropped image size should be (enforcing square) should prob be 1024, 1080, or 1200\n",
    "\n",
    "augmentation = None # Options are \"log\" \"sqrt\" None\n",
    "\n",
    "assert padding in [\"single\", \"double\"], f\"Padding must be 'single' or 'double', not {padding}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the dataset from scratch if needed\n",
    "\n",
    "Assumes you want all moving to one fixed frame\n",
    "\n",
    "TODO: Reconsider this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Folder_20250219120831_RFa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:33<00:00, 573.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 frames for Folder_20250219120831_RFa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if raw:\n",
    "    assert frames_per_dataset\n",
    "    n_frames = 2000 # How many frames are in each tif file\n",
    "    channel = 1 # Which channel to use (1 in RFa, 0 in normal - check)\n",
    "    input_files = {\n",
    "                    # \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250219110008_RFa_swim\" : [\n",
    "                    #                 \"20250219_Experiment_01_0-1999.tif\",\n",
    "                    #                 \"20250219_Experiment_01_2000-3999.tif\",\n",
    "                    #                 \"20250219_Experiment_01_4000-5999.tif\",\n",
    "                    #                 \"20250219_Experiment_01_8000-9999.tif\",\n",
    "                    #                 \"20250219_Experiment_01_6000-7999.tif\"],\n",
    "                    \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250219120831_RFa\": [\n",
    "                                    \"RIG_20250219_Experiment_01_0-1999.tif\",\n",
    "                                    \"RIG_20250219_Experiment_01_2000-3999.tif\",\n",
    "                                    \"RIG_20250219_Experiment_01_4000-5999.tif\",\n",
    "                                    \"RIG_20250219_Experiment_01_6000-7999.tif\",\n",
    "                                    \"RIG_20250219_Experiment_01_8000-9999.tif\"],\n",
    "                    # \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250214153740_RFa\": [\n",
    "                    #                 \"20250214_Experiment_01_0-1999.tif\",\n",
    "                    #                 \"20250214_Experiment_01_2000-3999.tif\",\n",
    "                    #                 \"20250214_Experiment_01_4000-5999.tif\"],\n",
    "                    }\n",
    "\n",
    "    unlabs = np.full((30, 2), -1)\n",
    "\n",
    "    with h5py.File(os.path.join(base_folder, \"moving_images.h5\"), 'w-') as h5m, h5py.File(os.path.join(base_folder, \"fixed_images.h5\"), 'w-') as h5f, h5py.File(\n",
    "            os.path.join(base_folder, \"moving_labels.h5\"), 'w-') as h5ml, h5py.File(os.path.join(base_folder, \"fixed_labels.h5\"), 'w-') as h5fl:\n",
    "        dataset_ind = 0\n",
    "        for dataset_name, file_list in tqdm(input_files.items()):\n",
    "            file_list = [os.path.join(dataset_name, f) for f in file_list]\n",
    "\n",
    "            dataset_name = os.path.basename(dataset_name)\n",
    "\n",
    "            print(f\"Processing {dataset_name}...\")\n",
    "            file_list = list(file_list)\n",
    "            random.shuffle(file_list)\n",
    "\n",
    "            total_frames = len(file_list) * n_frames\n",
    "            if frames_per_dataset > total_frames:\n",
    "                raise ValueError(f\"Requested {frames_per_dataset} frames, but only {total_frames} available in {dataset_name}.\")\n",
    "\n",
    "            # Sample global frame indices\n",
    "            selected_global_indices = sorted(random.sample(range(total_frames), frames_per_dataset))\n",
    "            selected_global_indices += manually_selected_frames\n",
    "            with open(os.path.join(base_folder, \"frame_log.txt\"), 'a') as f: # Save a log of the indices\n",
    "                f.write(f\"# {dataset_name}\\n\")\n",
    "                for idx in selected_global_indices:\n",
    "                    f.write(f\"{idx}\\n\")\n",
    "\n",
    "            # Iterate and extract frames\n",
    "            current_global_index = 0\n",
    "            saved_count = 0\n",
    "            for tif_path in file_list:\n",
    "                start, end = map(int, tif_path.split(\"_\")[-1].replace(\".tif\", \"\").split(\"-\"))\n",
    "\n",
    "                with tifffile.TiffFile(tif_path) as tif:\n",
    "                    arr = tif.asarray()\n",
    "                    if arr.ndim != 4:\n",
    "                        raise ValueError(f\"Expected shape (T, C, H, W) but got {arr.shape} in {tif_path}\")\n",
    "                    \n",
    "                    if start == 0:\n",
    "                        # Save the fixed image\n",
    "                        h5f.create_dataset(f\"{dataset_ind}_0to{dataset_ind}_0\", data=arr[0, channel])\n",
    "                        h5fl.create_dataset(f\"{dataset_ind}_0to{dataset_ind}_0\", data=unlabs)\n",
    "\n",
    "                    for idx in selected_global_indices:\n",
    "                        if idx >= start and idx <= end:\n",
    "                            local_idx = idx - start\n",
    "                            frame = arr[local_idx, channel]  # shape (H, W)\n",
    "                            ds_name = f\"{dataset_ind}_{idx}to{dataset_ind}_0\" # We're going to add the first number to diferentiate the datasets\n",
    "                            h5m.create_dataset(ds_name, data=frame)\n",
    "                            h5ml.create_dataset(ds_name, data=unlabs)\n",
    "\n",
    "                            saved_count += 1\n",
    "\n",
    "            dataset_ind += 1\n",
    "            print(f\"Saved {saved_count} frames for {dataset_name}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone fixed image to train\n",
    "Start with images (formatted as just 1 fixed image from Weissbourd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_images.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_images.h5\")\n",
    "moving_h5 = os.path.join(base_folder, \"moving_images.h5\")\n",
    "with h5py.File(old_fixed_h5, 'r') as f, h5py.File(moving_h5, 'r') as g, h5py.File(new_fixed_h5, 'w-') as fo:\n",
    "    if len(f.keys()) == 1:\n",
    "        img = f[list(f.keys())[0]][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = img)\n",
    "    else:\n",
    "        assert len(set([f.split(\"_\")[-1] for f in f.keys()])) == 1, \"Expected a file with either one image or one image per dataset\"\n",
    "        base_imgs = {}\n",
    "        for im in f.keys():\n",
    "            base_imgs[im.split(\"_\")[0]] = f[im][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = base_imgs[prob.split(\"_\")[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the same for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_labels.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_labels.h5\")\n",
    "moving_h5 = os.path.join(base_folder, \"moving_labels.h5\")\n",
    "\n",
    "with h5py.File(old_fixed_h5, 'r') as f, h5py.File(moving_h5, 'r') as g, h5py.File(new_fixed_h5, 'w-') as fo:\n",
    "    if len(f.keys()) == 1:\n",
    "        labs = f[list(f.keys())[0]][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = labs)\n",
    "    else:\n",
    "        assert len(set([f.split(\"_\")[-1] for f in f.keys()])) == 1, \"Expected a file with either one image or one image per dataset\"\n",
    "        base_labs = {}\n",
    "        for im in f.keys():\n",
    "            base_labs[im.split(\"_\")[0]] = f[im][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = base_labs[prob.split(\"_\")[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a third column so the labels have three dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_labels.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_fixed_labels.h5\")\n",
    "with h5py.File(old_fixed_h5, 'r') as f:\n",
    "    assert f[list(f.keys())[0]][:].shape[1] == 2, \"This is meant to expand the labels from 2D to 3D\"\n",
    "    with h5py.File(new_fixed_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            labs = f[prob][:]\n",
    "            labs = np.concatenate((labs, np.zeros((labs.shape[0], 1))), 1)\n",
    "            labs = np.pad(labs, [(0,label_shape[0] - labs.shape[0]), (0,0)], \"constant\", constant_values=-1)\n",
    "            g.create_dataset(prob, data = labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_moving_h5 = os.path.join(base_folder, \"moving_labels.h5\")\n",
    "new_moving_h5 = os.path.join(base_folder, \"fixed_moving_labels.h5\")\n",
    "with h5py.File(old_moving_h5, 'r') as f:\n",
    "    assert f[list(f.keys())[0]][:].shape[1] == 2, \"This is meant to expand the labels from 2D to 3D\"\n",
    "    with h5py.File(new_moving_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            labs = f[prob][:]\n",
    "            labs = np.concatenate((labs, np.zeros((labs.shape[0], 1))), 1)\n",
    "            labs = np.pad(labs, [(0,label_shape[0] - labs.shape[0]), (0,0)], \"constant\", constant_values=-1)\n",
    "            g.create_dataset(prob, data = labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add a third dim for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_images.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_fixed_images.h5\")\n",
    "with h5py.File(old_fixed_h5, 'r') as f:\n",
    "    assert len(f[list(f.keys())[0]][:].shape) == 2, \"This is meant to expand the images from 2D to 3D\"\n",
    "    with h5py.File(new_fixed_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            img = f[prob][:]\n",
    "            img = np.expand_dims(img, 2)\n",
    "            g.create_dataset(prob, data = img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_moving_h5 = os.path.join(base_folder, \"moving_images.h5\")\n",
    "new_moving_h5 = os.path.join(base_folder, \"fixed_moving_images.h5\")\n",
    "with h5py.File(old_moving_h5, 'r') as f:\n",
    "    assert len(f[list(f.keys())[0]][:].shape) == 2, \"This is meant to expand the images from 2D to 3D\"\n",
    "    with h5py.File(new_moving_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            img = f[prob][:]\n",
    "            img = np.expand_dims(img, 2)\n",
    "            g.create_dataset(prob, data = img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take unlabeled frames and add to a labeled h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_extra:\n",
    "\n",
    "    old_img_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/fixed_images.h5\"\n",
    "    new_img_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/fixed_images.h5\"\n",
    "\n",
    "    old_label_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/fixed_labels.h5\"\n",
    "    new_label_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/fixed_labels.h5\"\n",
    "\n",
    "    old_img_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/moving_images.h5\"\n",
    "    new_img_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/moving_images.h5\"\n",
    "\n",
    "    old_label_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/moving_labels.h5\"\n",
    "    new_label_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/moving_labels.h5\"\n",
    "\n",
    "    frame_folder = \"/home/brian/data4/brian/PBnJ/jelly_centroid_prep/zoomed_in_vid\"\n",
    "\n",
    "\n",
    "    frame_names = [\n",
    "        os.path.splitext(p)[0] for p in os.listdir(frame_folder)\n",
    "        if os.path.splitext(p)[-1] in [\".jpg\"]\n",
    "    ]\n",
    "\n",
    "    probs = []\n",
    "\n",
    "    for i in range(num_frames_to_add):\n",
    "        probs.append(tuple(random.sample(frame_names, 2)))\n",
    "\n",
    "    # old_mean = 0\n",
    "    old_max = 0\n",
    "    # num = 0\n",
    "\n",
    "    with h5py.File(new_img_fixed_h5, 'w-') as nif:\n",
    "        with h5py.File(new_label_fixed_h5, 'w-') as nlf:\n",
    "            with h5py.File(old_img_fixed_h5, 'r') as oif:\n",
    "                with h5py.File(old_label_fixed_h5, 'r') as olf:\n",
    "                    for prob in oif.keys():\n",
    "                        img = oif[prob][:]\n",
    "                        labs = olf[prob][:]\n",
    "\n",
    "                        # old_mean += np.mean(img)\n",
    "                        old_max = max(old_max, np.max(img))\n",
    "                        # num += 1\n",
    "\n",
    "                        nif.create_dataset(prob, data = img, dtype=float)\n",
    "                        nlf.create_dataset(prob, data = labs, dtype=np.float32)\n",
    "            \n",
    "            # old_mean = old_mean / num\n",
    "            scale_factor = old_max / 255 # Photo max val\n",
    "            \n",
    "            unlabeled_labs = np.ones_like(labs, dtype=np.float32) * -1\n",
    "            for fixed, moving in probs:\n",
    "                prob = f\"{moving}to{fixed}\"\n",
    "                img = cv2.imread(os.path.join(frame_folder, fixed + \".jpg\"))[:,:,0:1]\n",
    "                nif.create_dataset(prob, data = img * scale_factor, dtype=float)\n",
    "                nlf.create_dataset(prob, data = unlabeled_labs, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(new_img_moving_h5, 'w-') as nim:\n",
    "        with h5py.File(new_label_moving_h5, 'w-') as nlm:\n",
    "            with h5py.File(old_img_moving_h5, 'r') as oim:\n",
    "                with h5py.File(old_label_moving_h5, 'r') as olm:\n",
    "                    for prob in oim.keys():\n",
    "                        img = oim[prob][:]\n",
    "                        labs = olm[prob][:]\n",
    "                        nim.create_dataset(prob, data = img, dtype=float)\n",
    "                        nlm.create_dataset(prob, data = labs, dtype=np.float32)\n",
    "\n",
    "            for fixed, moving in probs:\n",
    "                prob = f\"{moving}to{fixed}\"\n",
    "                img = cv2.imread(os.path.join(frame_folder, moving + \".jpg\"))[:,:,0:1]\n",
    "                nim.create_dataset(prob, data = img * scale_factor, dtype=float)\n",
    "                nlm.create_dataset(prob, data = unlabeled_labs, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = base_folder\n",
    "out_dir = os.path.join(base_folder, \"cropped\")\n",
    "\n",
    "crop_shape = np.array((side_len, side_len, 1))\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with h5py.File(os.path.join(in_dir, \"fixed_moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"fixed_moving_labels.h5\"), 'r') as lmv, h5py.File(\n",
    "    os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "    with h5py.File(os.path.join(in_dir, \"fixed_fixed_fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_fixed_fixed_labels.h5\"), 'r') as lfx, h5py.File(\n",
    "        os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "        for prob in imv.keys():\n",
    "            img = imv[prob][:]\n",
    "            imgF = ifx[prob][:]\n",
    "\n",
    "            crop_offset = (img.shape - crop_shape) / 2\n",
    "            assert np.all(crop_offset == crop_offset.astype(int))\n",
    "            crop_offset = crop_offset.astype(int)\n",
    "\n",
    "            ## Crop the images\n",
    "            img = img[crop_offset[0]:crop_offset[0] + crop_shape[0], crop_offset[1]:crop_offset[1] + crop_shape[1], crop_offset[2]:crop_offset[2] + crop_shape[2]]\n",
    "            assert np.all(img.shape == crop_shape)\n",
    "\n",
    "            imgF = imgF[crop_offset[0]:crop_offset[0] + crop_shape[0], crop_offset[1]:crop_offset[1] + crop_shape[1], crop_offset[2]:crop_offset[2] + crop_shape[2]]\n",
    "            assert np.all(imgF.shape == crop_shape), imgF.shape\n",
    "\n",
    "            ## Adjust the labels\n",
    "            labs = lmv[prob][:]\n",
    "            neg_ones = labs < 0\n",
    "            labs = labs - crop_offset \n",
    "\n",
    "            labsF = lfx[prob][:]\n",
    "            neg_onesF = labsF < 0\n",
    "            labsF = labsF - crop_offset\n",
    "            assert np.all(neg_ones == neg_onesF), \"Fixed and Moving labels have a different number of non negative 1 labels\"\n",
    "            \n",
    "            # Remove centroids that are cropped out of the image\n",
    "            crop = np.logical_or(labs < 0, labs >= crop_shape)\n",
    "            crop = np.max(crop, axis=-1)\n",
    "            cropF = np.logical_or(labsF < 0, labsF >= crop_shape)\n",
    "            cropF = np.max(cropF, axis=-1)\n",
    "            crop = np.logical_or(crop, cropF) # If either the fixed or moving are out of bounds then exclude both\n",
    "\n",
    "            labs[crop] = -1\n",
    "            labsF[crop] = -1\n",
    "\n",
    "            labs[neg_ones] = -1 # Retain -1s\n",
    "            labsF[neg_ones] = -1 # Retain -1s\n",
    "\n",
    "            # Double check that all of the out of frame centroids are gone\n",
    "            assert np.all(np.logical_or(labs >= 0, labs == -1)), \"The crop results in moving centroids that are under bounds\"\n",
    "            assert np.all(labs < crop_shape[0]), \"The crop results in moving centroids that are over bounds\"\n",
    "            assert np.all(np.logical_or(labsF >= 0, labsF == -1)), \"The crop results in fixed centroids that are under bounds\"\n",
    "            assert np.all(labsF < crop_shape[0]), \"The crop results in fixed centroids that are over bounds\"\n",
    "\n",
    "            assert np.all((labs == -1) == (labsF == -1)), \"The labels that are excluded are not the same between moving and fixed\"\n",
    "\n",
    "\n",
    "            oimv.create_dataset(prob, data = img)\n",
    "            olmv.create_dataset(prob, data = labs)\n",
    "            oifx.create_dataset(prob, data = imgF)\n",
    "            olfx.create_dataset(prob, data = labsF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = os.path.join(base_folder, \"cropped\")\n",
    "out_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "if padding == \"single\":\n",
    "    padding = np.array([[0,0],[0,0],[1,0]])\n",
    "else:\n",
    "    padding = np.array([[0,0],[0,0],[1,1]])\n",
    "\n",
    "with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"moving_labels.h5\"), 'r') as lmv:\n",
    "    with h5py.File(os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "        for prob in imv.keys():\n",
    "            img = imv[prob][:]\n",
    "            # img = np.pad(img, padding, \"constant\", constant_values=0)\n",
    "            img = np.pad(img, padding, \"constant\", constant_values=np.min(img))\n",
    "\n",
    "            labs = lmv[prob][:]\n",
    "            neg_ones = labs < 0\n",
    "            labs = labs + padding[:, 0] \n",
    "            labs[neg_ones] = -1 # Retain -1s\n",
    "\n",
    "            oimv.create_dataset(prob, data = img)\n",
    "            olmv.create_dataset(prob, data = labs)\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(in_dir, \"fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_labels.h5\"), 'r') as lfx:\n",
    "    with h5py.File(os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "        for prob in ifx.keys():\n",
    "            img = ifx[prob][:]\n",
    "            # img = np.pad(img, padding, \"constant\", constant_values=0)\n",
    "            img = np.pad(img, padding, \"constant\", constant_values=np.min(img))\n",
    "\n",
    "            labs = lfx[prob][:]\n",
    "            neg_ones = labs < 0\n",
    "            labs = labs + padding[:, 0] \n",
    "            labs[neg_ones] = -1 # Retain -1s\n",
    "            \n",
    "            oifx.create_dataset(prob, data = img)\n",
    "            olfx.create_dataset(prob, data = labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceiling\n",
    "#### log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation == \"log\":\n",
    "\n",
    "    in_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "    out_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"log_scaled\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"moving_labels.h5\"), 'r') as lmv:\n",
    "        with h5py.File(os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "            for prob in imv.keys():\n",
    "                img = imv[prob][:]\n",
    "                labs = lmv[prob][:]\n",
    "                \n",
    "                img = np.log2(img - (np.min(img) - 1), dtype=np.float32)\n",
    "                # img = np.log(img + 1, dtype=np.float32)\n",
    "\n",
    "                oimv.create_dataset(prob, data = img)\n",
    "                olmv.create_dataset(prob, data = labs)\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_labels.h5\"), 'r') as lfx:\n",
    "        with h5py.File(os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "            for prob in ifx.keys():\n",
    "                img = ifx[prob][:]\n",
    "                labs = lfx[prob][:]\n",
    "                \n",
    "                img = np.log2(img - (np.min(img) - 1), dtype=np.float32)\n",
    "                # img = np.log(img + 1, dtype=np.float32)\n",
    "                \n",
    "                oifx.create_dataset(prob, data = img)\n",
    "                olfx.create_dataset(prob, data = labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation == \"sqrt\":\n",
    "\n",
    "    in_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "    out_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"sqrt_scaled\")\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"moving_labels.h5\"), 'r') as lmv:\n",
    "        with h5py.File(os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "            for prob in imv.keys():\n",
    "                img = imv[prob][:]\n",
    "                labs = lmv[prob][:]\n",
    "                \n",
    "                img = np.sqrt(img - np.min(img), dtype=np.float32)\n",
    "\n",
    "                oimv.create_dataset(prob, data = img)\n",
    "                olmv.create_dataset(prob, data = labs)\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_labels.h5\"), 'r') as lfx:\n",
    "        with h5py.File(os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "            for prob in ifx.keys():\n",
    "                img = ifx[prob][:]\n",
    "                labs = lfx[prob][:]\n",
    "                \n",
    "                img = np.sqrt(img - np.min(img), dtype=np.float32)\n",
    "                \n",
    "                oifx.create_dataset(prob, data = img)\n",
    "                olfx.create_dataset(prob, data = labs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_1008to0_0', '0_102to0_0', '0_1098to0_0', '0_1101to0_0', '0_1110to0_0', '0_1122to0_0', '0_1204to0_0', '0_1225to0_0', '0_1234to0_0', '0_1279to0_0', '0_1394to0_0', '0_1424to0_0', '0_1438to0_0', '0_1595to0_0', '0_1600to0_0', '0_163to0_0', '0_1640to0_0', '0_1658to0_0', '0_1845to0_0', '0_1930to0_0', '0_1988to0_0', '0_198to0_0', '0_1991to0_0', '0_1to0_0', '0_2007to0_0', '0_2009to0_0', '0_2115to0_0', '0_2119to0_0', '0_2147to0_0', '0_2149to0_0', '0_2152to0_0', '0_2179to0_0', '0_2323to0_0', '0_2381to0_0', '0_2486to0_0', '0_2491to0_0', '0_2503to0_0', '0_2607to0_0', '0_2634to0_0', '0_2638to0_0', '0_2646to0_0', '0_2722to0_0', '0_2762to0_0', '0_2815to0_0', '0_281to0_0', '0_2916to0_0', '0_3061to0_0', '0_3072to0_0', '0_3106to0_0', '0_3113to0_0', '0_3144to0_0', '0_3283to0_0', '0_3297to0_0', '0_3299to0_0', '0_3334to0_0', '0_334to0_0', '0_3361to0_0', '0_3365to0_0', '0_336to0_0', '0_3387to0_0', '0_3429to0_0', '0_3462to0_0', '0_3504to0_0', '0_3623to0_0', '0_3625to0_0', '0_3665to0_0', '0_3671to0_0', '0_3689to0_0', '0_3807to0_0', '0_3830to0_0', '0_3954to0_0', '0_3959to0_0', '0_3961to0_0', '0_4019to0_0', '0_4141to0_0', '0_4237to0_0', '0_4298to0_0', '0_4342to0_0', '0_4344to0_0', '0_4389to0_0', '0_449to0_0', '0_4571to0_0', '0_458to0_0', '0_4650to0_0', '0_4684to0_0', '0_4788to0_0', '0_4799to0_0', '0_4815to0_0', '0_4919to0_0', '0_4949to0_0', '0_495to0_0', '0_4979to0_0', '0_497to0_0', '0_4985to0_0', '0_5030to0_0', '0_5048to0_0', '0_5082to0_0', '0_5208to0_0', '0_520to0_0', '0_5296to0_0', '0_5431to0_0', '0_543to0_0', '0_5509to0_0', '0_5548to0_0', '0_5573to0_0', '0_567to0_0', '0_575to0_0', '0_5847to0_0', '0_5863to0_0', '0_5875to0_0', '0_5911to0_0', '0_6075to0_0', '0_6090to0_0', '0_6101to0_0', '0_6111to0_0', '0_6131to0_0', '0_6149to0_0', '0_6158to0_0', '0_6167to0_0', '0_6258to0_0', '0_6316to0_0', '0_636to0_0', '0_6467to0_0', '0_6478to0_0', '0_647to0_0', '0_6546to0_0', '0_6551to0_0', '0_6558to0_0', '0_6583to0_0', '0_6584to0_0', '0_6647to0_0', '0_6664to0_0', '0_6821to0_0', '0_6895to0_0', '0_6902to0_0', '0_6968to0_0', '0_697to0_0', '0_7036to0_0', '0_705to0_0', '0_710to0_0', '0_7136to0_0', '0_7172to0_0', '0_7186to0_0', '0_7246to0_0', '0_7299to0_0', '0_72to0_0', '0_7305to0_0', '0_7353to0_0', '0_739to0_0', '0_7612to0_0', '0_7663to0_0', '0_7681to0_0', '0_7848to0_0', '0_7879to0_0', '0_802to0_0', '0_8034to0_0', '0_8091to0_0', '0_8105to0_0', '0_8341to0_0', '0_8361to0_0', '0_8402to0_0', '0_8417to0_0', '0_8463to0_0', '0_8470to0_0', '0_8717to0_0', '0_8753to0_0', '0_8926to0_0', '0_8980to0_0', '0_8992to0_0', '0_9012to0_0', '0_902to0_0', '0_9033to0_0', '0_9103to0_0', '0_9104to0_0', '0_9149to0_0', '0_9154to0_0', '0_9195to0_0', '0_9233to0_0', '0_9259to0_0', '0_9282to0_0', '0_9300to0_0', '0_9334to0_0', '0_9349to0_0', '0_9356to0_0', '0_9366to0_0', '0_9476to0_0', '0_9494to0_0', '0_951to0_0', '0_9598to0_0', '0_9651to0_0', '0_966to0_0', '0_9755to0_0', '0_9796to0_0', '0_9821to0_0', '0_9826to0_0', '0_9877to0_0', '0_9882to0_0', '0_9889to0_0', '0_9984to0_0', '0_9992to0_0']\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as f:\n",
    "    probs = list(f.keys())\n",
    "    print(probs)\n",
    "    print(len(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into training and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_647to0_0', '0_1640to0_0', '0_7681to0_0', '0_3959to0_0', '0_3954to0_0', '0_9233to0_0', '0_6258to0_0', '0_8105to0_0', '0_6647to0_0', '0_543to0_0', '0_575to0_0', '0_8926to0_0', '0_2916to0_0', '0_5863to0_0', '0_902to0_0', '0_3061to0_0', '0_1234to0_0', '0_9494to0_0', '0_6558to0_0', '0_710to0_0', '0_6467to0_0', '0_4684to0_0', '0_739to0_0', '0_3671to0_0', '0_2634to0_0', '0_4949to0_0', '0_1600to0_0', '0_5431to0_0', '0_3830to0_0', '0_3689to0_0']\n"
     ]
    }
   ],
   "source": [
    "val_probs = random.sample(probs, 30)\n",
    "print(val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs = ['0_647to0_0', '0_1640to0_0', '0_7681to0_0', '0_3959to0_0', '0_3954to0_0', '0_9233to0_0', '0_6258to0_0', '0_8105to0_0', '0_6647to0_0', '0_543to0_0', '0_575to0_0', '0_8926to0_0', '0_2916to0_0', '0_5863to0_0', '0_902to0_0', '0_3061to0_0', '0_1234to0_0', '0_9494to0_0', '0_6558to0_0', '0_710to0_0', '0_6467to0_0', '0_4684to0_0', '0_739to0_0', '0_3671to0_0', '0_2634to0_0', '0_4949to0_0', '0_1600to0_0', '0_5431to0_0', '0_3830to0_0', '0_3689to0_0']\n",
    "\n",
    "if augmentation == None:\n",
    "    base_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "elif augmentation == \"sqrt\":\n",
    "    base_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"sqrt_scaled\")    \n",
    "elif augmentation == \"log\":\n",
    "    base_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"log_scaled\") \n",
    "else:\n",
    "    raise ValueError(f\"Unknown augmentation {augmentation}\")\n",
    "\n",
    "os.mkdir(os.path.join(base_dir, \"train\"))\n",
    "os.mkdir(os.path.join(base_dir, \"val\"))\n",
    "\n",
    "old_img_fixed_h5 = f\"{base_dir}/fixed_images.h5\"\n",
    "train_img_fixed_h5 = f\"{base_dir}/train/fixed_images.h5\"\n",
    "val_img_fixed_h5 = f\"{base_dir}/val/fixed_images.h5\"\n",
    "\n",
    "old_label_fixed_h5 = f\"{base_dir}/fixed_labels.h5\"\n",
    "train_label_fixed_h5 = f\"{base_dir}/train/fixed_labels.h5\"\n",
    "val_label_fixed_h5 = f\"{base_dir}/val/fixed_labels.h5\"\n",
    "\n",
    "old_img_moving_h5 = f\"{base_dir}/moving_images.h5\"\n",
    "train_img_moving_h5 = f\"{base_dir}/train/moving_images.h5\"\n",
    "val_img_moving_h5 = f\"{base_dir}/val/moving_images.h5\"\n",
    "\n",
    "old_label_moving_h5 = f\"{base_dir}/moving_labels.h5\"\n",
    "train_label_moving_h5 = f\"{base_dir}/train/moving_labels.h5\"\n",
    "val_label_moving_h5 = f\"{base_dir}/val/moving_labels.h5\"\n",
    "\n",
    "\n",
    "with h5py.File(train_img_fixed_h5, 'w-') as tif,  h5py.File(val_img_fixed_h5, 'w-') as vif:\n",
    "    with h5py.File(train_label_fixed_h5, 'w-') as tlf,  h5py.File(val_label_fixed_h5, 'w-') as vlf:\n",
    "        with h5py.File(old_img_fixed_h5, 'r') as oif:\n",
    "            with h5py.File(old_label_fixed_h5, 'r') as olf:\n",
    "                for prob in oif.keys():\n",
    "                    img = oif[prob][:]\n",
    "                    labs = olf[prob][:]\n",
    "                    if prob in val_probs:\n",
    "                        vif.create_dataset(prob, data = img)\n",
    "                        vlf.create_dataset(prob, data = labs)\n",
    "                    else:\n",
    "                        tif.create_dataset(prob, data = img)\n",
    "                        tlf.create_dataset(prob, data = labs)\n",
    "        \n",
    "\n",
    "with h5py.File(train_img_moving_h5, 'w-') as tim,  h5py.File(val_img_moving_h5, 'w-') as vim:\n",
    "    with h5py.File(train_label_moving_h5, 'w-') as tlm,  h5py.File(val_label_moving_h5, 'w-') as vlm:\n",
    "        with h5py.File(old_img_moving_h5, 'r') as oim:\n",
    "            with h5py.File(old_label_moving_h5, 'r') as olm:\n",
    "                for prob in oim.keys():\n",
    "                    img = oim[prob][:]\n",
    "                    labs = olm[prob][:]\n",
    "                    if prob in val_probs:\n",
    "                        vim.create_dataset(prob, data = img)\n",
    "                        vlm.create_dataset(prob, data = labs)\n",
    "                    else:\n",
    "                        tim.create_dataset(prob, data = img)\n",
    "                        tlm.create_dataset(prob, data = labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_img_moving_h5 = f\"{base_dir}/moving_images.h5\"\n",
    "\n",
    "train_roi_moving_h5 = f\"{base_dir}/train/moving_rois.h5\"\n",
    "val_roi_moving_h5 = f\"{base_dir}/val/moving_rois.h5\"\n",
    "train_roi_fixed_h5 = f\"{base_dir}/train/fixed_rois.h5\"\n",
    "val_roi_fixed_h5 = f\"{base_dir}/val/fixed_rois.h5\"\n",
    "\n",
    "\n",
    "with h5py.File(val_roi_fixed_h5, 'w-') as vrf,  h5py.File(val_roi_moving_h5, 'w-') as vrm:\n",
    "# with h5py.File(train_roi_fixed_h5, 'w-') as trf,  h5py.File(train_roi_moving_h5, 'w-') as trm:\n",
    "    with h5py.File(old_img_moving_h5, 'r') as oim:\n",
    "        for prob in oim.keys():\n",
    "            blank = np.zeros_like(oim[prob][:])\n",
    "            vrf.create_dataset(prob, data = blank)\n",
    "            vrm.create_dataset(prob, data = blank)\n",
    "            # trf.create_dataset(prob, data = blank)\n",
    "            # trm.create_dataset(prob, data = blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
