{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1f37bd",
   "metadata": {},
   "source": [
    "# CellDiscoveryNet training notebook\n",
    "\n",
    "This notebook assumes you have already run the `make_CellDiscoveryNet_input` notebook. It trains a CellDiscoveryNet model on the data generated by that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888ebcb-fa32-4e10-8461-cefda81e9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # adjust which GPU you want to use here\n",
    "\n",
    "from deepreg.util import build_dataset\n",
    "from deepreg.registry import REGISTRY\n",
    "from deepreg.model import layer, layer_util\n",
    "import deepreg.loss as loss\n",
    "# from wormalign.network import DDFNetworkTrainer, DDFNetworkTester\n",
    "import deepreg.model.optimizer as opt\n",
    "from deepreg.callback import build_checkpoint_callback\n",
    "\n",
    "import deepreg.predict as predict\n",
    "import deepreg.train as train\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import nrrd\n",
    "import nibabel as nib\n",
    "\n",
    "from deepreg.loss import image as img_loss\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e308b3-14ad-4803-b2e8-72dcaa1bffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batched_image(batched_image, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Normalizes each image in a batch to [0, 1] range separately.\n",
    "    \"\"\"\n",
    "    # Calculate the min and max values for each image in the batch\n",
    "    min_vals = tf.math.reduce_min(batched_image, axis=[1, 2, 3], keepdims=True)\n",
    "    max_vals = tf.math.reduce_max(batched_image, axis=[1, 2, 3], keepdims=True)\n",
    "    # Normalize each image separately\n",
    "    batched_image = batched_image - min_vals\n",
    "    batched_image = batched_image / tf.maximum(max_vals - min_vals, eps)\n",
    "    return batched_image\n",
    "\n",
    "def compute_centroids_3d(image, max_val):\n",
    "    \"\"\"\n",
    "    Compute the centroids of all pixels with each unique value in a 3D image.\n",
    "\n",
    "    :param image: A 3D numpy array representing the image with dimensions (x, y, z).\n",
    "    :return: A Nx3 numpy array, where N is the maximum value in the image plus one.\n",
    "             Each row corresponds to the centroid coordinates (x, y, z) for each value.\n",
    "    \"\"\"\n",
    "    centroids = np.zeros((max_val, 3), dtype=np.float32) - 1  # Initialize the centroids array\n",
    "    for val in range(1,max_val + 1):\n",
    "        # Find the indices of pixels that have the current value\n",
    "        indices = np.argwhere(image == val)\n",
    "        # Compute the centroid if the value is present in the image\n",
    "        if len(indices) > 0:\n",
    "            centroid_x = np.mean(indices[:, 0])  # x-coordinate\n",
    "            centroid_y = np.mean(indices[:, 1])  # y-coordinate\n",
    "            centroid_z = np.mean(indices[:, 2])  # z-coordinate\n",
    "            centroids[val-1] = [centroid_x, centroid_y, centroid_z]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efb333-b5d4-4f79-8a26-39ea61eb5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a5eda",
   "metadata": {},
   "source": [
    "## Initialize network configuration settings\n",
    "\n",
    "Make sure to edit the `dir` settings in the YAML configuration file for `train`, `valid`, and `test` to point to the location of your data. Each `dir` should point to a directory containing `fixed_images.h5` and `moving_images.h5` files output by the `make_CellDiscoveryNet_input` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab66118-a6f7-4213-93be-0cf36c120bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/store1/PublishedData/Data/prj_register/CellDiscoveryNet/train_config.yaml\"\n",
    "log_dir = \"/path/to/your/log_dir\"\n",
    "ckpt_path = \"\"\n",
    "exp_name = \"multicolor_gncc\"\n",
    "max_epochs = 600\n",
    "\n",
    "config, log_dir, ckpt_path = train.build_config(\n",
    "    config_path=config_path,\n",
    "    log_dir=log_dir,\n",
    "    exp_name=exp_name,\n",
    "    ckpt_path=ckpt_path,\n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "\n",
    "batch_size = config[\"train\"][\"preprocess\"][\"batch_size\"]\n",
    "\n",
    "batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2eed2d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d464bbd-cd1e-43c8-a498-68a063f8fd1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### build datasets\n",
    "\n",
    "data_loader_train, dataset_train, steps_per_epoch_train = build_dataset(\n",
    "    dataset_config=config[\"dataset\"],\n",
    "    preprocess_config=config[\"train\"][\"preprocess\"],\n",
    "    split=\"train\",\n",
    "    training=True,\n",
    "    repeat=True,\n",
    ")\n",
    "\n",
    "data_loader_val, dataset_val, steps_per_epoch_val = build_dataset(\n",
    "    dataset_config=config[\"dataset\"],\n",
    "    preprocess_config=config[\"train\"][\"preprocess\"],\n",
    "    split=\"valid\",\n",
    "    training=False,\n",
    "    repeat=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059144",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model\n",
    "\n",
    "model: tf.keras.Model = REGISTRY.build_model(\n",
    "    config=dict(\n",
    "        name=config[\"train\"][\"method\"],\n",
    "        moving_image_size=data_loader_train.moving_image_shape,\n",
    "        fixed_image_size=data_loader_train.fixed_image_shape,\n",
    "        moving_label_size=(200,3),\n",
    "        fixed_label_size=(200,3),\n",
    "        index_size=data_loader_train.num_indices,\n",
    "        labeled=config[\"dataset\"][\"train\"][\"labeled\"],\n",
    "        batch_size=batch_size,\n",
    "        config=config[\"train\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284127cf",
   "metadata": {},
   "source": [
    "## Build optimizer and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = opt.build_optimizer(optimizer_config=config[\"train\"][\"optimizer\"])\n",
    "model.compile(optimizer=optimizer)\n",
    "model.plot_model(output_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf580596",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=config[\"train\"][\"save_period\"],\n",
    "    update_freq=config[\"train\"].get(\"update_freq\", \"epoch\"),\n",
    ")\n",
    "ckpt_callback, initial_epoch = build_checkpoint_callback(\n",
    "    model=model,\n",
    "    dataset=dataset_train,\n",
    "    log_dir=log_dir,\n",
    "    save_period=config[\"train\"][\"save_period\"],\n",
    "    ckpt_path=ckpt_path,\n",
    ")\n",
    "callbacks = [tensorboard_callback, ckpt_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f78c",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=dataset_train,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    initial_epoch=300,\n",
    "    epochs=config[\"train\"][\"epochs\"],\n",
    "    validation_data=dataset_val,\n",
    "    validation_steps=steps_per_epoch_val,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/path/to/your/history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeed899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Julia conda)",
   "language": "python",
   "name": "your-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
