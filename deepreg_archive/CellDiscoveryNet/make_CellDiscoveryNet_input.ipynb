{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data for CellDiscoveryNet\n",
    "\n",
    "This notebook creates training data for the CellDiscoveryNet model. It assumes you've already run the `make_AutoCellLabeler_input` notebook to create 4D H5 files. Notably, it is important that `θh_pos_is_ventral` is set correctly in that notebook to align the images. It is not necessary for those H5 files to have human labels - as CellDiscoveryNet uses unsupervised learning, any labels will be discarded.\n",
    "\n",
    "This notebook then aligns those H5 files using manual rotation to align the axes, followed by GPU-accelerated Euler registration.\n",
    "\n",
    "The output of this notebook is available in [our Dropbox](https://www.dropbox.com/scl/fo/ealblchspq427pfmhtg7h/ALZ7AE5o3bT0VUQ8TTeR1As?rlkey=1e6tseyuwd04rbj7wmn2n6ij7&e=1&st=ybsvv0ry&dl=0) in the `CellDiscoveryNet` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"CUDA_VISIBLE_DEVICES\"] = 0 # set GPU to use\n",
    "\n",
    "using ImageRegistration\n",
    "\n",
    "using HDF5\n",
    "\n",
    "using ImageDataIO\n",
    "using PyCall\n",
    "using NRRDIO\n",
    "using Statistics\n",
    "using ImageRegistration\n",
    "using FlavellBase\n",
    "using ProgressMeter\n",
    "using MultivariateStats\n",
    "using PyPlot\n",
    "using ProgressMeter\n",
    "using Images\n",
    "\n",
    "using JLD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_prj_neuropal = [\"2022-07-15-06\", \"2022-07-15-12\", \"2022-07-20-01\", \"2022-07-26-01\", \"2022-08-02-01\", \"2023-01-23-08\", \"2023-01-23-15\", \"2023-01-23-21\", \"2023-01-19-08\", \"2023-01-19-22\", \"2023-01-09-28\", \"2023-01-17-01\", \"2023-01-19-15\", \"2023-01-23-01\", \"2023-03-07-01\", \"2022-12-21-06\", \"2023-01-05-18\", \"2023-01-06-01\", \"2023-01-06-08\", \"2023-01-09-08\", \"2023-01-09-15\", \"2023-01-09-22\", \"2023-01-10-07\", \"2023-01-10-14\", \"2023-01-13-07\", \"2023-01-16-01\", \"2023-01-16-08\", \"2023-01-16-15\", \"2023-01-16-22\", \"2023-01-17-07\", \"2023-01-17-14\", \"2023-01-18-01\"]\n",
    "datasets_prj_rim = [\"2023-06-09-01\", \"2023-07-28-04\", \"2023-06-24-02\", \"2023-07-07-11\", \"2023-08-07-01\", \"2023-06-24-11\", \"2023-07-07-18\", \"2023-08-18-11\", \"2023-06-24-28\", \"2023-07-11-02\", \"2023-08-22-08\", \"2023-07-12-01\", \"2023-07-01-09\", \"2023-07-13-01\", \"2023-06-09-10\", \"2023-07-07-01\", \"2023-08-07-16\", \"2023-08-22-01\", \"2023-08-23-23\", \"2023-08-25-02\", \"2023-09-15-01\", \"2023-09-15-08\", \"2023-08-18-18\", \"2023-08-19-01\", \"2023-08-23-09\", \"2023-08-25-09\", \"2023-09-01-01\", \"2023-08-31-03\", \"2023-07-01-01\", \"2023-07-01-23\"]\n",
    "\n",
    "datasets_prj_aversion = [\"2023-03-30-01\", \"2023-06-29-01\", \"2023-06-29-13\", \"2023-07-14-08\", \"2023-07-14-14\", \"2023-07-27-01\", \"2023-08-08-07\", \"2023-08-14-01\", \"2023-08-16-01\", \"2023-08-21-01\", \"2023-09-07-01\", \"2023-09-14-01\", \"2023-08-15-01\", \"2023-10-05-01\", \"2023-06-23-08\", \"2023-12-11-01\", \"2023-06-21-01\"]\n",
    "datasets_prj_5ht = [\"2022-07-26-31\", \"2022-07-26-38\", \"2022-07-27-31\", \"2022-07-27-38\", \"2022-07-27-45\", \"2022-08-02-31\", \"2022-08-02-38\", \"2022-08-03-31\"]\n",
    "datasets_prj_starvation = [\"2023-05-25-08\", \"2023-05-26-08\", \"2023-06-05-10\", \"2023-06-05-17\", \"2023-07-24-27\", \"2023-09-27-14\", \"2023-05-25-01\", \"2023-05-26-01\", \"2023-07-24-12\", \"2023-07-24-20\", \"2023-09-12-01\", \"2023-09-19-01\", \"2023-09-29-19\", \"2023-10-09-01\", \"2023-09-13-02\"]\n",
    "\n",
    "# append all datasets togther\n",
    "datasets = []\n",
    "append!(datasets, datasets_prj_neuropal)\n",
    "append!(datasets, datasets_prj_rim)\n",
    "append!(datasets, datasets_prj_aversion)\n",
    "append!(datasets, datasets_prj_5ht)\n",
    "append!(datasets, datasets_prj_starvation)\n",
    "\n",
    "datasets_val = [\"2023-06-24-02\", \"2023-08-07-01\", \"2023-08-19-01\", # RIM datasets\n",
    "                \"2022-07-26-01\", \"2023-01-23-21\", \"2023-01-23-01\", # NeuroPAL datasets\n",
    "                \"2023-07-14-08\", # Aversion datasets\n",
    "                \"2022-08-02-31\", # 5-HT datasets\n",
    "                \"2023-07-24-27\", \"2023-07-24-20\"] # Starvation datasets\n",
    "datasets_test = [\"2023-08-22-01\", \"2023-07-07-18\", \"2023-07-01-23\",  # RIM datasets\n",
    "                 \"2023-01-06-01\", \"2023-01-10-07\", \"2023-01-17-07\", # Neuropal datasets\n",
    "                 \"2023-08-21-01\", \"2023-06-23-08\", # Aversion datasets\n",
    "                 \"2022-07-27-38\", # 5-HT datasets\n",
    "                 \"2023-10-09-01\", \"2023-09-13-02\" # Starvation datasets\n",
    "                 ]\n",
    "datasets_train = [dataset for dataset in datasets if !(dataset in datasets_val) && !(dataset in datasets_test)]\n",
    "\n",
    "datasets_ = deepcopy(datasets_train)\n",
    "append!(datasets_, datasets_val)\n",
    "append!(datasets_, datasets_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from HDF5 files\n",
    "function load_images(datasets, base_path; key=\"raw\")\n",
    "    imgs = []\n",
    "    for dataset in datasets\n",
    "        path_img = base_path * dataset * \".h5\"\n",
    "        img = nothing\n",
    "        h5open(path_img, \"r\") do f\n",
    "            img = read(f[key])\n",
    "        end\n",
    "        push!(imgs, img)\n",
    "    end\n",
    "    return imgs\n",
    "end\n",
    "\n",
    "# Function to rotate 4D image by 180 degrees in the xy plane\n",
    "function rotate_image_180_xy(img)\n",
    "    for z in 1:size(img, 3)\n",
    "        for ch in 1:size(img, 4)\n",
    "            img[:, :, z, ch] = rot180(img[:, :, z, ch])\n",
    "        end\n",
    "    end\n",
    "    return img\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually align the images\n",
    "\n",
    "This notebook assumes the images are already aligned about the xz axis, which is done automatically by the `make_AutoCellLabeler_input` notebook using the `θh_pos_is_ventral` parameter. However, the xy axis may be misaligned. We will manually align the images by rotating them about the z axis. To do this for your data, check where the heads of each of the animals are and set `rotate_idx_train` to the set of images with the uncommon head orientation to rotate those images by 180 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_train = load_images(datasets_train, \"/path/to/your/input/data/train\")\n",
    "imgs_val = load_images(datasets_val, \"/path/to/your/input/data/val\")\n",
    "imgs_test = load_images(datasets_test, \"/path/to/your/input/data/test\")\n",
    "imgs_roi = load_images(datasets_, \"/path/to/your/input/data/roi_crop/\", key=\"roi\");\n",
    "\n",
    "root_path = \"/path/to/your/output/data/CellDiscoveryNet\"\n",
    "\n",
    "create_dir(root_path)\n",
    "rotate_idx_train = [5, 62];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate specified images by 180 degrees in the xy plane for training data\n",
    "for idx in rotate_idx_train\n",
    "    imgs_train[idx] = rotate_image_180_xy(imgs_train[idx])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_all = deepcopy(imgs_train)\n",
    "append!(imgs_all, imgs_val)\n",
    "append!(imgs_all, imgs_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images per channel\n",
    "for i in 1:length(imgs_all)\n",
    "    img = Float64.(imgs_all[i])\n",
    "    for j in 1:4\n",
    "        img[:,:,:,j] ./= maximum(img[:,:,:,j])\n",
    "    end\n",
    "    imgs_all[i] = img\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in rotate_idx_train\n",
    "    imgs_roi[idx] = rotate_image_180_xy(imgs_roi[idx])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_component = Dict(\n",
    "    \"train\" => datasets_train,\n",
    "    \"val\" => datasets_val,\n",
    "    \"test\" => datasets_test\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euler register images\n",
    "\n",
    "This registers each pair of images in the xy plane using the GPU-accelerated Euler registration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_parameters = Dict()\n",
    "np = pyimport(\"numpy\")\n",
    "\n",
    "euler_x_translation_range_1 = np.sort(np.concatenate((\n",
    "    np.linspace(-0.24, 0.24, 49),\n",
    "    np.linspace(-0.46, -0.25, 8),\n",
    "    np.linspace(0.25, 0.46, 8),\n",
    "    np.linspace(0.5, 1, 3),\n",
    "    np.linspace(-1, -0.5, 3))))\n",
    "\n",
    "euler_y_translation_range_1 = np.sort(np.concatenate((\n",
    "    np.linspace(-0.28, 0.28, 29),\n",
    "    np.linspace(-0.54, -0.3, 5),\n",
    "    np.linspace(0.3, 0.54, 5),\n",
    "    np.linspace(0.6, 1.4, 3),\n",
    "    np.linspace(-1.4, -0.6, 3))))\n",
    "\n",
    "euler_theta_rotation_range_xy = np.sort(np.concatenate((\n",
    "    np.linspace(0, 19, 20),\n",
    "    np.linspace(341, 359, 19)))) # disallow 180 degree rotations since we manually handled those earlier\n",
    "\n",
    "memory_dict = nothing\n",
    "memory_dict_3d = nothing\n",
    "\n",
    "euler_gpu = pyimport(\"euler_gpu\")\n",
    "pytorch = pyimport(\"torch\")\n",
    "\n",
    "device = pytorch.device(\"cuda:0\")\n",
    "\n",
    "BATCH_SIZE = 5000 # reduce this if you run out of GPU memory\n",
    "\n",
    "@showprogress for (j, fixed_dataset) in enumerate(datasets_)\n",
    "    fixed_image = imgs_all[j]\n",
    "    transposed_fixed_image = permutedims(fixed_image, (4, 3, 2, 1))\n",
    "\n",
    "    create_dir(joinpath(root_path, \"img_fixed\"))\n",
    "    fixed_img_file = joinpath(root_path, \"img_fixed/$(j).h5\")\n",
    "    h5write(fixed_img_file, \"raw\", transposed_fixed_image)\n",
    "\n",
    "    path = joinpath(root_path, \"roi_fixed\")\n",
    "    create_dir(path)    \n",
    "    h5open(joinpath(path, \"$(j).h5\"), \"w\") do f\n",
    "        img = imgs_roi[j]\n",
    "        write(f, \"roi\", permutedims(img, (3,2,1))) # make dimensions consistent with images\n",
    "    end\n",
    "\n",
    "    for (i, moving_dataset) in enumerate(datasets_)\n",
    "        if i >= j \n",
    "            continue\n",
    "        end\n",
    "        moving_image = imgs_all[i]\n",
    "\n",
    "        @assert(size(moving_image)[1] % 2 == 0 && size(moving_image)[2] % 2 == 0 && size(moving_image) == size(fixed_image))\n",
    "\n",
    "        moving_image_downsampled = euler_gpu.max_intensity_projection_and_downsample(moving_image[:,:,:,4], 4, 2)\n",
    "        fixed_image_downsampled = euler_gpu.max_intensity_projection_and_downsample(fixed_image[:,:,:,4], 4, 2)\n",
    "\n",
    "        if isnothing(memory_dict)\n",
    "            memory_dict = euler_gpu.initialize(fixed_image_downsampled, moving_image_downsampled, euler_x_translation_range_1, euler_y_translation_range_1, euler_theta_rotation_range_xy, BATCH_SIZE, device)\n",
    "        else\n",
    "            memory_dict[\"moving_images_repeated\"] = pytorch.tensor(moving_image_downsampled, device=device, dtype=pytorch.float32).unsqueeze(0).repeat(BATCH_SIZE,1,1,1);\n",
    "            memory_dict[\"fixed_images_repeated\"] = pytorch.tensor(fixed_image_downsampled, device=device, dtype=pytorch.float32).unsqueeze(0).repeat(BATCH_SIZE,1,1,1);\n",
    "        end\n",
    "\n",
    "        best_score, best_transformation = euler_gpu.grid_search(memory_dict)\n",
    "\n",
    "        z_dim = size(fixed_image)[3]\n",
    "\n",
    "        if isnothing(memory_dict_3d)\n",
    "            memory_dict_3d = euler_gpu.initialize(fixed_image[:,:,1,4], moving_image[:,:,1,4], zeros(z_dim), zeros(z_dim), zeros(z_dim), z_dim, device);\n",
    "        end\n",
    "\n",
    "        moving_image_tensor = pytorch.tensor(permutedims(moving_image[:,:,:,4], [3, 1, 2]), device=device, dtype=pytorch.float32).unsqueeze(1).repeat(1,1,1,1);\n",
    "        moving_image_transformed = euler_gpu.transform_image(moving_image_tensor, (best_transformation[1]).repeat(z_dim), best_transformation[2].repeat(z_dim), best_transformation[3].repeat(z_dim), memory_dict_3d);\n",
    "        moving_image_transformed_cpu = permutedims(dropdims(moving_image_transformed.cpu().numpy(), dims=2), [2,3,1]);\n",
    "        shift_range = collect(-40:40)\n",
    "        dz, gncc, moving_image_transformed_z = euler_gpu.translate_along_z(shift_range, fixed_image[:,:,:,1], moving_image_transformed_cpu, 0)\n",
    "\n",
    "        euler_parameters[(i, j)] = ([tfm.cpu().numpy() for tfm in best_transformation], dz)\n",
    "\n",
    "        transformed_moving_image = zeros(size(moving_image)...)\n",
    "\n",
    "        for ch = 1:4\n",
    "            moving_image_tensor = pytorch.tensor(permutedims(moving_image[:,:,:,ch], [3, 1, 2]), device=device, dtype=pytorch.float32).unsqueeze(1).repeat(1,1,1,1);\n",
    "            moving_image_transformed = euler_gpu.transform_image(moving_image_tensor, (best_transformation[1]).repeat(z_dim), best_transformation[2].repeat(z_dim), best_transformation[3].repeat(z_dim), memory_dict_3d);\n",
    "            moving_image_transformed_cpu = permutedims(dropdims(moving_image_transformed.cpu().numpy(), dims=2), [2,3,1]);\n",
    "            moving_image_transformed_z = translate_z(moving_image_transformed_cpu, dz, 0.0)\n",
    "\n",
    "            transformed_moving_image[:,:,:,ch] .= moving_image_transformed_z\n",
    "        end\n",
    "\n",
    "        # Save the transformed_moving_image and fixed_image\n",
    "        transposed_moving_image = permutedims(transformed_moving_image, (4, 3, 2, 1))\n",
    "\n",
    "\n",
    "        for component in [\"train\", \"val\", \"test\"]\n",
    "            # note that in ANTSUN 2U we use all registrations, including ones not localized to one component\n",
    "            if moving_dataset in datasets_component[component] && fixed_dataset in datasets_component[component]\n",
    "                create_dir(joinpath(root_path, \"$(component)\"))\n",
    "                moving_img_file = joinpath(root_path, \"$(component)/moving_images.h5\")\n",
    "                fixed_img_file = joinpath(root_path, \"$(component)/fixed_images.h5\")\n",
    "                h5write(moving_img_file, \"dataset_$(i)_$(j)\", transposed_moving_image)\n",
    "                h5write(fixed_img_file, \"dataset_$(i)_$(j)\", transposed_fixed_image)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        create_dir(joinpath(root_path, \"euler_tfm_moving\"))\n",
    "        moving_img_file = joinpath(root_path, \"euler_tfm_moving/$(i)_$(j).h5\")\n",
    "        h5write(moving_img_file, \"raw\", transposed_moving_image)\n",
    "\n",
    "        moving_roi_image = np.array(imgs_roi[i], dtype=np.int32)\n",
    "        \n",
    "        z_dim = size(moving_roi_image)[3]\n",
    "        if isnothing(memory_dict_3d)\n",
    "            memory_dict_3d = euler_gpu.initialize(moving_roi_image[:,:,1], moving_roi_image[:,:,1], zeros(z_dim), zeros(z_dim), zeros(z_dim), z_dim, device);\n",
    "        end\n",
    "\n",
    "        moving_roi_image_tensor = pytorch.tensor(permutedims(moving_roi_image[:,:,:], [3, 1, 2]), device=device, dtype=pytorch.float32).unsqueeze(1).repeat(1,1,1,1);\n",
    "        moving_roi_image_transformed = euler_gpu.transform_image(moving_roi_image_tensor, (best_transformation[1]).repeat(z_dim), best_transformation[2].repeat(z_dim), best_transformation[3].repeat(z_dim), memory_dict_3d, interpolation=\"nearest\");\n",
    "        moving_roi_image_transformed_cpu = permutedims(dropdims(moving_roi_image_transformed.cpu().numpy(), dims=2), [2,3,1]);\n",
    "        moving_roi_image_transformed_z = translate_z(moving_roi_image_transformed_cpu, dz, 0.0)\n",
    "\n",
    "        # Save the transformed_moving_roi_image and fixed_image\n",
    "        transposed_moving_roi_image = permutedims(moving_roi_image_transformed_z, (3, 2, 1))\n",
    "\n",
    "        create_dir(joinpath(root_path, \"euler_tfm_moving_roi\"))\n",
    "        moving_roi_img_file = joinpath(root_path, \"euler_tfm_moving_roi/$(i)_to_$(j).h5\")\n",
    "        h5write(moving_roi_img_file, \"roi\", transposed_moving_roi_image)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save registration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JLD2.@save(joinpath(root_path, \"euler_parameters.jld2\"), euler_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_parameters = JLD2.load(joinpath(root_path, \"euler_parameters.jld2\"))[\"euler_parameters\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
